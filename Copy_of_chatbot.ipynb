{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naina1905/chatbot/blob/main/Copy_of_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "SgwXx2qnt5oh",
        "outputId": "efab4287-42f3-44b9-9ba3-156e1a93d7f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/bot (1).json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-82396a156d98>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# reading the json.intense file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mintents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/bot (1).json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# creating empty lists to store data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/bot (1).json'"
          ]
        }
      ],
      "source": [
        "# importing the required modules.\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from keras.models import Sequential\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# reading the json.intense file\n",
        "intents = json.loads(open(\"/bot (1).json\").read())\n",
        "\n",
        "# creating empty lists to store data\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_letters = [\"?\", \"!\", \".\", \",\"]\n",
        "for intent in intents['intents']:\n",
        "\tfor pattern in intent['patterns']:\n",
        "\t\t# separating words from patterns\n",
        "\t\tword_list = nltk.word_tokenize(pattern)\n",
        "\t\twords.extend(word_list) # and adding them to words list\n",
        "\n",
        "\t\t# associating patterns with respective tags\n",
        "\t\tdocuments.append(((word_list), intent['tag']))\n",
        "\n",
        "\t\t# appending the tags to the class list\n",
        "\t\tif intent['tag'] not in classes:\n",
        "\t\t\tclasses.append(intent['tag'])\n",
        "\n",
        "# storing the root words or lemma\n",
        "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
        "words = sorted(set(words))\n",
        "\n",
        "# saving the words and classes list to binary files\n",
        "pickle.dump(words, open('words.pkl', 'wb'))\n",
        "pickle.dump(classes, open('classes.pkl', 'wb'))\n",
        "# we need numerical values of the\n",
        "# words because a neural network\n",
        "# needs numerical values to work with\n",
        "training = []\n",
        "output_empty = [0]*len(classes)\n",
        "for document in documents:\n",
        "\tbag = []\n",
        "\tword_patterns = document[0]\n",
        "\tword_patterns = [lemmatizer.lemmatize(\n",
        "\t\tword.lower()) for word in word_patterns]\n",
        "\tfor word in words:\n",
        "\t\tbag.append(1) if word in word_patterns else bag.append(0)\n",
        "\n",
        "\t# making a copy of the output_empty\n",
        "\toutput_row = list(output_empty)\n",
        "\toutput_row[classes.index(document[1])] = 1\n",
        "\ttraining.append([bag, output_row])\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# splitting the data\n",
        "train_x = list(training[:, 0])\n",
        "train_y = list(training[:, 1])\n",
        "# creating a Sequential machine learning model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]), ),\n",
        "\t\t\t\tactivation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]),\n",
        "\t\t\t\tactivation='softmax'))\n",
        "\n",
        "# compiling the model\n",
        "sgd = SGD(lr=0.01,  momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "hist = model.fit(np.array(train_x), np.array(train_y),\n",
        "\t\t\t\tepochs=200, batch_size=5, verbose=1)\n",
        "\n",
        "# saving the model\n",
        "model.save(\"chatbotmodel.h5\", hist)\n",
        "\n",
        "# print statement to show the\n",
        "# successful training of the Chatbot model\n",
        "print(\"Yay!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJlrOvdtiojh"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgV6wTpnuLoK",
        "outputId": "952d3734-583d-448b-a127-fa9bf6176652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot is up!\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "Hi! I'm Restrobot. How may I assist you today?\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nltk\n",
        "from keras.models import load_model\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "intents = json.loads(open(\"/bot (1).json\").read())\n",
        "words = pickle.load(open('words.pkl', 'rb'))\n",
        "classes = pickle.load(open('classes.pkl', 'rb'))\n",
        "model = load_model('chatbotmodel.h5')\n",
        "\n",
        "def clean_up_sentences(sentence):\n",
        "\tsentence_words = nltk.word_tokenize(sentence)\n",
        "\tsentence_words = [lemmatizer.lemmatize(word)\n",
        "\t\t\t\t\tfor word in sentence_words]\n",
        "\treturn sentence_words\n",
        "\n",
        "def bagw(sentence):\n",
        "\tsentence_words = clean_up_sentences(sentence)\n",
        "\tbag = [0]*len(words)\n",
        "\tfor w in sentence_words:\n",
        "\t\tfor i, word in enumerate(words):\n",
        "\t\t\tif word == w:\n",
        "\t\t\t\tbag[i] = 1\n",
        "\treturn np.array(bag)\n",
        "\n",
        "def predict_class(sentence):\n",
        "\tbow = bagw(sentence)\n",
        "\tres = model.predict(np.array([bow]))[0]\n",
        "\tERROR_THRESHOLD = 0.25\n",
        "\tresults = [[i, r] for i, r in enumerate(res)\n",
        "\t\t\tif r > ERROR_THRESHOLD]\n",
        "\tresults.sort(key=lambda x: x[1], reverse=True)\n",
        "\treturn_list = []\n",
        "\tfor r in results:\n",
        "\t\treturn_list.append({'intent': classes[r[0]],\n",
        "\t\t\t\t\t\t\t'probability': str(r[1])})\n",
        "\t\treturn return_list\n",
        "\n",
        "def get_response(intents_list, intents_json):\n",
        "\ttag = intents_list[0]['intent']\n",
        "\tlist_of_intents = intents_json['intents']\n",
        "\tresult = \"\"\n",
        "\tfor i in list_of_intents:\n",
        "\t\tif i['tag'] == tag:\n",
        "\t\t\tresult = random.choice(i['responses'])\n",
        "\t\t\tbreak\n",
        "\treturn result\n",
        "\n",
        "print(\"Chatbot is up!\")\n",
        "\n",
        "while True:\n",
        "\tmessage = input(\"\")\n",
        "\tints = predict_class(message)\n",
        "\tres = get_response(ints, intents)\n",
        "\tprint(res)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}